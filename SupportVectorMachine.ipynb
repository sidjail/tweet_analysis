{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Cell involved importing the two datasets, merging them into one dataframe, shuffling the data\n",
    "and partitioning the data into training and test data with a 0.20 test proportion. Both the training and test data are further partitioned into X and y components, denoting each entries (tweets) and labels (0 or 1) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gen = pd.read_csv('genuine1.csv')\n",
    "poli = pd.read_csv('Political_Cleaned1.csv')\n",
    "df0 = pd.concat([gen,poli],axis=0,sort=True)\n",
    "df = df0.drop(['1'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['label'], test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply feature extraction to the X data. To do this we use a TfidfVectorizer which extracts a word dictionary from the data and creates a sparse matrix where for each row (data entry) we have the column indicating the prevelance of a specific word. The term frequency–inverse document frequency makes it so that rare words are weighted higher than frequent ones. Our new features allow us to work numerically since X has been tranformed from rows of tweets into rows of floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118797, 66171)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train.values.astype('U'))\n",
    "vector = vectorizer.transform(X_train.values.astype('U'))\n",
    "test_vector = vectorizer.transform(X_test.values.astype('U'))\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large dataset and the variety of words (66,145) create a very sparse matrix. So we use TruncatedSVD, in this context a form of LSA (Latent Semantic Analysis). This allows us to reduce the dimension to the specified number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Components [10000]\n",
      "Explained Variance Ratio [0.9467]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "n_components = [10000]\n",
    "explained_variance= [0]\n",
    "v_reduced = []\n",
    "test_v_reduced = []\n",
    "for i in range(len(n_components)):\n",
    "    n = n_components[i]\n",
    "    tsvd = TruncatedSVD(n_components=n, n_iter=5, random_state=32)\n",
    "    vector_reduced = tsvd.fit_transform(vector)\n",
    "    test_vector_reduced = tsvd.fit_transform(test_vector)\n",
    "    v_reduced.append(vector_reduced)\n",
    "    test_v_reduced.append(test_vector_reduced)\n",
    "    explained_v = tsvd.explained_variance_ratio_[0:n].sum()\n",
    "    explained_variance[i] = round(explained_v,4)\n",
    "\n",
    "print(\"N_Components\",n_components)\n",
    "print(\"Explained Variance Ratio\",explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v_reduced[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"truncatedTrain10000.csv\", v_reduced[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "svm = LinearSVC(C=10)\n",
    "for n in range(len(n_components)):\n",
    "    clf = svm.fit(v_reduced[n],y_train)\n",
    "    p1 = clf.predict(v_reduced[n])\n",
    "    p2 = clf.predict(test_v_reduced[n])\n",
    "    accTrain = metrics.accuracy_score(p1,y_train)\n",
    "    accTest = metrics.accuracy_score(p2,y_test)\n",
    "    print(\"N_Components\",n_components[n])\n",
    "    print(\"Train Accuracy\",accTrain)\n",
    "    print(\"Test Accuracy\", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply a linear Support Vector Machine on the reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1.0\n",
      "0.9643593693443437\n",
      "0.9046127946127946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "C = np.logspace(0, 4, 10)\n",
    "hyperparameters = dict(C=C)\n",
    "svm = LinearSVC()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(svm, hyperparameters, cv=5, verbose=0)\n",
    "model = clf.fit(vector,y_train)\n",
    "print('Best C:', model.best_estimator_.get_params()['C'])\n",
    "pred = model.predict(vector)\n",
    "trainAcc = metrics.accuracy_score(y_train,pred)\n",
    "print(trainAcc)\n",
    "testPred = model.predict(test_vector)\n",
    "testAcc = metrics.accuracy_score(y_test,testPred)\n",
    "print(testAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(vector,y_train)\n",
    "p = clf.predict(vector)\n",
    "p2 = clf.predict(test_vector)\n",
    "from sklearn import metrics\n",
    "acc = metrics.accuracy_score(y_train,p)\n",
    "testacc = metrics.accuracy_score(y_test,p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc 0.9644687997171646\n",
      "Test Acc 0.9045791245791246\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Acc\", acc)\n",
    "print(\"Test Acc\", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’]\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel = 'poly')\n",
    "clf.fit(vector,y_train)\n",
    "p = clf.predict(vector)\n",
    "p2 = clf.predict(test_vector)\n",
    "from sklearn import metrics\n",
    "acc = metrics.accuracy_score(y_train,p)\n",
    "testacc = metrics.accuracy_score(y_test,p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
