{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Conv2D, MaxPool2D, Conv1D, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data known to be fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"Political_Cleaned - Political_Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data known to be genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"genuine - election_clean_withouturlandemoticons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling if necessary, set to 1 to use whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1Sample = data1.sample(frac = 1)\n",
    "data2Sample = data2.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFull = data1Sample.append(data2Sample, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148497 entries, 0 to 148496\n",
      "Data columns (total 3 columns):\n",
      "content    148493 non-null object\n",
      "label      148497 non-null int64\n",
      "count      148497 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dataFull.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataFull.content.astype(str)\n",
    "Y = dataFull.label\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the test train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
    "x_train = []\n",
    "for each in X_train:\n",
    "    x_train.append(each)\n",
    "x_test = []\n",
    "for each in X_test:\n",
    "    x_test.append(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenising\n",
    "The vocabulary size is fixed at 10000 words.<br>\n",
    "The most frequent words are used to build a dictionary which is used to encode each sentence.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_len = 150\n",
    "tok = Tokenizer(max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "tok.fit_on_texts(x_train)\n",
    "sequences = tok.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dictionary and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'to',\n",
       " 3: 'in',\n",
       " 4: 'a',\n",
       " 5: 'of',\n",
       " 6: 'for',\n",
       " 7: 'you',\n",
       " 8: 'and',\n",
       " 9: 'is',\n",
       " 10: 'electionday',\n",
       " 11: 'rt',\n",
       " 12: 'on',\n",
       " 13: 'trump',\n",
       " 14: 'i',\n",
       " 15: 'debate',\n",
       " 16: 'this',\n",
       " 17: 'be',\n",
       " 18: 'vote',\n",
       " 19: 'are',\n",
       " 20: 'at',\n",
       " 21: 'your',\n",
       " 22: 'it',\n",
       " 23: 'will',\n",
       " 24: 'with',\n",
       " 25: 'that',\n",
       " 26: 'debatenight',\n",
       " 27: 'not',\n",
       " 28: 'we',\n",
       " 29: 'hillary',\n",
       " 30: 'have',\n",
       " 31: 'all',\n",
       " 32: 'about',\n",
       " 33: 'out',\n",
       " 34: 'if',\n",
       " 35: 'what',\n",
       " 36: 'from',\n",
       " 37: 'as',\n",
       " 38: 'my',\n",
       " 39: 'up',\n",
       " 40: 'get',\n",
       " 41: 'who',\n",
       " 42: 'do',\n",
       " 43: 'by',\n",
       " 44: 'tonight',\n",
       " 45: 'debates',\n",
       " 46: 'clinton',\n",
       " 47: 'so',\n",
       " 48: 'us',\n",
       " 49: 'today',\n",
       " 50: 'people',\n",
       " 51: 'just',\n",
       " 52: \"it's\",\n",
       " 53: 'her',\n",
       " 54: 'news',\n",
       " 55: 'debates2016',\n",
       " 56: 'no',\n",
       " 57: \"don't\",\n",
       " 58: 'our',\n",
       " 59: 'america',\n",
       " 60: 'he',\n",
       " 61: 'how',\n",
       " 62: 'has',\n",
       " 63: 'but',\n",
       " 64: 'like',\n",
       " 65: 'can',\n",
       " 66: 'go',\n",
       " 67: 'they',\n",
       " 68: 'day',\n",
       " 69: 'u',\n",
       " 70: 'or',\n",
       " 71: 'one',\n",
       " 72: 'was',\n",
       " 73: 'make',\n",
       " 74: 'she',\n",
       " 75: 'his',\n",
       " 76: 'more',\n",
       " 77: 'after',\n",
       " 78: 'now',\n",
       " 79: 'new',\n",
       " 80: 'time',\n",
       " 81: 'president',\n",
       " 82: 'know',\n",
       " 83: 'an',\n",
       " 84: 'me',\n",
       " 85: 'when',\n",
       " 86: 'good',\n",
       " 87: 'election',\n",
       " 88: '2',\n",
       " 89: 'why',\n",
       " 90: \"i'm\",\n",
       " 91: 'man',\n",
       " 92: 'sanjose',\n",
       " 93: 'donald',\n",
       " 94: 'watch',\n",
       " 95: 'should',\n",
       " 96: 'over',\n",
       " 97: 'imwithher',\n",
       " 98: 'need',\n",
       " 99: 'politics',\n",
       " 100: 'please',\n",
       " 101: 'going',\n",
       " 102: 'would',\n",
       " 103: 'right',\n",
       " 104: 'want',\n",
       " 105: 'here',\n",
       " 106: 'obama',\n",
       " 107: 'presidential',\n",
       " 108: 'police',\n",
       " 109: 'see',\n",
       " 110: 'voting',\n",
       " 111: 'polls',\n",
       " 112: 'win',\n",
       " 113: 'only',\n",
       " 114: 'there',\n",
       " 115: 'maga',\n",
       " 116: '4',\n",
       " 117: 'san',\n",
       " 118: 'video',\n",
       " 119: 'their',\n",
       " 120: 'first',\n",
       " 121: 'say',\n",
       " 122: 'great',\n",
       " 123: 'says',\n",
       " 124: \"can't\",\n",
       " 125: 'world',\n",
       " 126: 'line',\n",
       " 127: 'election2016',\n",
       " 128: 'than',\n",
       " 129: 'think',\n",
       " 130: 'them',\n",
       " 131: 'take',\n",
       " 132: 'country',\n",
       " 133: 'off',\n",
       " 134: 'american',\n",
       " 135: 'live',\n",
       " 136: '1',\n",
       " 137: 'did',\n",
       " 138: 'him',\n",
       " 139: 'back',\n",
       " 140: 'been',\n",
       " 141: \"you're\",\n",
       " 142: 'woman',\n",
       " 143: 'into',\n",
       " 144: \"tonight's\",\n",
       " 145: 'never',\n",
       " 146: 'media',\n",
       " 147: 'last',\n",
       " 148: 'because',\n",
       " 149: 'before',\n",
       " 150: 'again',\n",
       " 151: 'some',\n",
       " 152: 'down',\n",
       " 153: 'americans',\n",
       " 154: 'love',\n",
       " 155: 'let',\n",
       " 156: 'open',\n",
       " 157: 'white',\n",
       " 158: 'campaign',\n",
       " 159: 'year',\n",
       " 160: 'even',\n",
       " 161: 'women',\n",
       " 162: 'way',\n",
       " 163: 'got',\n",
       " 164: 'too',\n",
       " 165: 'help',\n",
       " 166: 'may',\n",
       " 167: 'state',\n",
       " 168: 'where',\n",
       " 169: 'still',\n",
       " 170: 'show',\n",
       " 171: 'really',\n",
       " 172: 'morning',\n",
       " 173: 'against',\n",
       " 174: 'being',\n",
       " 175: 'best',\n",
       " 176: 'could',\n",
       " 177: 'stop',\n",
       " 178: 'w',\n",
       " 179: 'am',\n",
       " 180: \"let's\",\n",
       " 181: 'breaking',\n",
       " 182: 'most',\n",
       " 183: 'look',\n",
       " 184: 'during',\n",
       " 185: 'old',\n",
       " 186: '3',\n",
       " 187: 'every',\n",
       " 188: 'voted',\n",
       " 189: 'sure',\n",
       " 190: 'next',\n",
       " 191: 'god',\n",
       " 192: 'years',\n",
       " 193: 'house',\n",
       " 194: 'black',\n",
       " 195: 'its',\n",
       " 196: 'these',\n",
       " 197: 'big',\n",
       " 198: 'hope',\n",
       " 199: 'final',\n",
       " 200: 'real',\n",
       " 201: 'much',\n",
       " 202: 'then',\n",
       " 203: 'had',\n",
       " 204: '5',\n",
       " 205: 'ready',\n",
       " 206: 'health',\n",
       " 207: 'many',\n",
       " 208: 'voters',\n",
       " 209: 'keep',\n",
       " 210: 'does',\n",
       " 211: 'usa',\n",
       " 212: 'were',\n",
       " 213: 's',\n",
       " 214: 'long',\n",
       " 215: \"that's\",\n",
       " 216: 'ask',\n",
       " 217: 'must',\n",
       " 218: 'any',\n",
       " 219: \"doesn't\",\n",
       " 220: 'poll',\n",
       " 221: 'happy',\n",
       " 222: 'watching',\n",
       " 223: 'said',\n",
       " 224: 'two',\n",
       " 225: 'already',\n",
       " 226: 'ever',\n",
       " 227: 'issues',\n",
       " 228: 'killed',\n",
       " 229: 'bill',\n",
       " 230: 'better',\n",
       " 231: 'come',\n",
       " 232: 'lies',\n",
       " 233: 'jose',\n",
       " 234: \"trump's\",\n",
       " 235: 'well',\n",
       " 236: 'home',\n",
       " 237: 'change',\n",
       " 238: 'work',\n",
       " 239: 'remember',\n",
       " 240: 'talk',\n",
       " 241: 'hrc',\n",
       " 242: 'fire',\n",
       " 243: '2016',\n",
       " 244: 'night',\n",
       " 245: 'support',\n",
       " 246: 'give',\n",
       " 247: 'life',\n",
       " 248: 'party',\n",
       " 249: 'tell',\n",
       " 250: 'polling',\n",
       " 251: 'other',\n",
       " 252: 'history',\n",
       " 253: 'everyone',\n",
       " 254: 'another',\n",
       " 255: 'care',\n",
       " 256: 'stay',\n",
       " 257: 'thing',\n",
       " 258: 'money',\n",
       " 259: 'debate2016',\n",
       " 260: 'candidates',\n",
       " 261: 'very',\n",
       " 262: 'gop',\n",
       " 263: 'needs',\n",
       " 264: 'believe',\n",
       " 265: 'made',\n",
       " 266: 'draintheswamp',\n",
       " 267: 'free',\n",
       " 268: '7',\n",
       " 269: 'place',\n",
       " 270: 'candidate',\n",
       " 271: 'thank',\n",
       " 272: 'things',\n",
       " 273: 'early',\n",
       " 274: \"he's\",\n",
       " 275: 'wait',\n",
       " 276: '6',\n",
       " 277: 'those',\n",
       " 278: 'use',\n",
       " 279: 'pjnet',\n",
       " 280: 'questions',\n",
       " 281: 'same',\n",
       " 282: '8',\n",
       " 283: 'california',\n",
       " 284: 'hillaryclinton',\n",
       " 285: 'game',\n",
       " 286: 'check',\n",
       " 287: 'bad',\n",
       " 288: 'arrested',\n",
       " 289: 'dead',\n",
       " 290: 'trumppence16',\n",
       " 291: 'school',\n",
       " 292: 'find',\n",
       " 293: 'wants',\n",
       " 294: 'high',\n",
       " 295: 'truth',\n",
       " 296: 'twitter',\n",
       " 297: 'while',\n",
       " 298: \"she's\",\n",
       " 299: 'end',\n",
       " 300: 'county',\n",
       " 301: 'call',\n",
       " 302: 'run',\n",
       " 303: 'r',\n",
       " 304: 'nothing',\n",
       " 305: 'luck',\n",
       " 306: 'voter',\n",
       " 307: 'wins',\n",
       " 308: 'bay',\n",
       " 309: 'bring',\n",
       " 310: \"won't\",\n",
       " 311: 'tcot',\n",
       " 312: 'family',\n",
       " 313: 'own',\n",
       " 314: 'mr',\n",
       " 315: 'getting',\n",
       " 316: '10',\n",
       " 317: 'supporters',\n",
       " 318: 'which',\n",
       " 319: 'done',\n",
       " 320: 'plan',\n",
       " 321: 'join',\n",
       " 322: 'city',\n",
       " 323: 'stand',\n",
       " 324: 'states',\n",
       " 325: 'shooting',\n",
       " 326: 'read',\n",
       " 327: 'under',\n",
       " 328: \"we're\",\n",
       " 329: 'put',\n",
       " 330: 'russia',\n",
       " 331: 'political',\n",
       " 332: 'found',\n",
       " 333: 'oh',\n",
       " 334: 'local',\n",
       " 335: \"'\",\n",
       " 336: 'matter',\n",
       " 337: 'yes',\n",
       " 338: 'question',\n",
       " 339: 'neverhillary',\n",
       " 340: \"didn't\",\n",
       " 341: 'million',\n",
       " 342: 'corrupt',\n",
       " 343: 'corruption',\n",
       " 344: 'fact',\n",
       " 345: 'ur',\n",
       " 346: 'tax',\n",
       " 347: 'away',\n",
       " 348: 'war',\n",
       " 349: 'also',\n",
       " 350: 'start',\n",
       " 351: \"isn't\",\n",
       " 352: 'gets',\n",
       " 353: 'attack',\n",
       " 354: 'job',\n",
       " 355: 'shot',\n",
       " 356: 'lol',\n",
       " 357: 'lying',\n",
       " 358: 'topvideo',\n",
       " 359: 'law',\n",
       " 360: 'death',\n",
       " 361: 'doing',\n",
       " 362: 'pay',\n",
       " 363: 'always',\n",
       " 364: 'gonna',\n",
       " 365: 'forget',\n",
       " 366: \"hillary's\",\n",
       " 367: 'isis',\n",
       " 368: 'jobs',\n",
       " 369: 'looking',\n",
       " 370: 'someone',\n",
       " 371: 'fraud',\n",
       " 372: 'little',\n",
       " 373: 'true',\n",
       " 374: 'finally',\n",
       " 375: 'rigged',\n",
       " 376: 'power',\n",
       " 377: 'democrats',\n",
       " 378: 'something',\n",
       " 379: 'vs',\n",
       " 380: 'florida',\n",
       " 381: 'both',\n",
       " 382: 'via',\n",
       " 383: 'hear',\n",
       " 384: 'emails',\n",
       " 385: 'security',\n",
       " 386: 'tv',\n",
       " 387: '000',\n",
       " 388: 'la',\n",
       " 389: 'anyone',\n",
       " 390: \"here's\",\n",
       " 391: 'person',\n",
       " 392: 'without',\n",
       " 393: 'hit',\n",
       " 394: 'thanks',\n",
       " 395: 'national',\n",
       " 396: 'around',\n",
       " 397: 'tweet',\n",
       " 398: 'liar',\n",
       " 399: 'potus',\n",
       " 400: 'lie',\n",
       " 401: '9',\n",
       " 402: 'car',\n",
       " 403: 'government',\n",
       " 404: 'left',\n",
       " 405: '30',\n",
       " 406: 'cnn',\n",
       " 407: 'crooked',\n",
       " 408: 'makes',\n",
       " 409: 'case',\n",
       " 410: 'fbi',\n",
       " 411: 'hate',\n",
       " 412: 'nevertrump',\n",
       " 413: 'report',\n",
       " 414: 'racist',\n",
       " 415: 'enough',\n",
       " 416: 'francisco',\n",
       " 417: 'fight',\n",
       " 418: \"i'll\",\n",
       " 419: 'everything',\n",
       " 420: 'yet',\n",
       " 421: 'crash',\n",
       " 422: 'anything',\n",
       " 423: 'times',\n",
       " 424: 'ahead',\n",
       " 425: 'elections2016',\n",
       " 426: 'kids',\n",
       " 427: 'n',\n",
       " 428: 'important',\n",
       " 429: 'top',\n",
       " 430: 'future',\n",
       " 431: 'friends',\n",
       " 432: 'feel',\n",
       " 433: 'since',\n",
       " 434: 'children',\n",
       " 435: 'wikileaks',\n",
       " 436: 'court',\n",
       " 437: 'coming',\n",
       " 438: 'myvote2016',\n",
       " 439: 'near',\n",
       " 440: 'between',\n",
       " 441: 'trying',\n",
       " 442: 'running',\n",
       " 443: 'wrong',\n",
       " 444: 'area',\n",
       " 445: 'calls',\n",
       " 446: 'girl',\n",
       " 447: 'policy',\n",
       " 448: 'gun',\n",
       " 449: 'crime',\n",
       " 450: 'votes',\n",
       " 451: 'south',\n",
       " 452: 'making',\n",
       " 453: 'voice',\n",
       " 454: 'through',\n",
       " 455: 'taxes',\n",
       " 456: 'trumps',\n",
       " 457: 'ballot',\n",
       " 458: 'electionnight',\n",
       " 459: 'podestaemails12',\n",
       " 460: 'mt',\n",
       " 461: 'santa',\n",
       " 462: 'lose',\n",
       " 463: 'until',\n",
       " 464: 'post',\n",
       " 465: 'actually',\n",
       " 466: 'suspect',\n",
       " 467: 'face',\n",
       " 468: 'topl',\n",
       " 469: 'days',\n",
       " 470: 'child',\n",
       " 471: 'heard',\n",
       " 472: 'public',\n",
       " 473: 'dont',\n",
       " 474: 'iv',\n",
       " 475: 'boy',\n",
       " 476: 'trumptrain',\n",
       " 477: 'part',\n",
       " 478: \"i've\",\n",
       " 479: 'latest',\n",
       " 480: 'benghazi',\n",
       " 481: 'head',\n",
       " 482: 'texas',\n",
       " 483: 'race',\n",
       " 484: 'rally',\n",
       " 485: 'answer',\n",
       " 486: 'ohio',\n",
       " 487: 'pence',\n",
       " 488: 'chris',\n",
       " 489: 'north',\n",
       " 490: 'choice',\n",
       " 491: 'b',\n",
       " 492: '1st',\n",
       " 493: 'men',\n",
       " 494: 'team',\n",
       " 495: 'follow',\n",
       " 496: 'full',\n",
       " 497: 'ppl',\n",
       " 498: 'crookedhillary',\n",
       " 499: 'looks',\n",
       " 500: 'united',\n",
       " 501: 'try',\n",
       " 502: 'vegas',\n",
       " 503: 'control',\n",
       " 504: 'business',\n",
       " 505: 'goes',\n",
       " 506: 'deal',\n",
       " 507: 'shit',\n",
       " 508: 'called',\n",
       " 509: 'immigration',\n",
       " 510: 'shows',\n",
       " 511: 'hey',\n",
       " 512: 'trust',\n",
       " 513: 'set',\n",
       " 514: 'office',\n",
       " 515: 'listen',\n",
       " 516: 'saying',\n",
       " 517: 'east',\n",
       " 518: 'might',\n",
       " 519: 't',\n",
       " 520: 'play',\n",
       " 521: 'yourself',\n",
       " 522: 'seen',\n",
       " 523: 'park',\n",
       " 524: 'decision2016',\n",
       " 525: 'talking',\n",
       " 526: 'week',\n",
       " 527: 'lines',\n",
       " 528: 'illegal',\n",
       " 529: 'republican',\n",
       " 530: \"there's\",\n",
       " 531: 'lot',\n",
       " 532: '20',\n",
       " 533: 'working',\n",
       " 534: 'wall',\n",
       " 535: 'syria',\n",
       " 536: 'point',\n",
       " 537: 'wake',\n",
       " 538: \"today's\",\n",
       " 539: 'tomorrow',\n",
       " 540: 'chance',\n",
       " 541: 'fuck',\n",
       " 542: 'fake',\n",
       " 543: 'waiting',\n",
       " 544: 'hell',\n",
       " 545: 'rights',\n",
       " 546: 'behind',\n",
       " 547: 'sunday',\n",
       " 548: 'former',\n",
       " 549: 'minutes',\n",
       " 550: 'name',\n",
       " 551: 'least',\n",
       " 552: 'missing',\n",
       " 553: 'words',\n",
       " 554: 'coverage',\n",
       " 555: 'hard',\n",
       " 556: 'muslim',\n",
       " 557: 'lost',\n",
       " 558: \"what's\",\n",
       " 559: 'stupid',\n",
       " 560: 'military',\n",
       " 561: 'oakland',\n",
       " 562: 'takes',\n",
       " 563: 'message',\n",
       " 564: 'sex',\n",
       " 565: 'speech',\n",
       " 566: 'lead',\n",
       " 567: 'policies',\n",
       " 568: 'wow',\n",
       " 569: 'nation',\n",
       " 570: '100',\n",
       " 571: 'officer',\n",
       " 572: 'mean',\n",
       " 573: 'chicago',\n",
       " 574: 'close',\n",
       " 575: 'save',\n",
       " 576: 'social',\n",
       " 577: 'system',\n",
       " 578: 'cast',\n",
       " 579: 'past',\n",
       " 580: 'thought',\n",
       " 581: 'record',\n",
       " 582: 'turn',\n",
       " 583: 'congress',\n",
       " 584: \"obama's\",\n",
       " 585: 'criminal',\n",
       " 586: 'bernie',\n",
       " 587: 'taking',\n",
       " 588: 'millions',\n",
       " 589: 'democracy',\n",
       " 590: 'far',\n",
       " 591: 'leader',\n",
       " 592: '15',\n",
       " 593: 'donaldtrump',\n",
       " 594: 'strongertogether',\n",
       " 595: 'sanders',\n",
       " 596: 'story',\n",
       " 597: 'dems',\n",
       " 598: 'economy',\n",
       " 599: 'ass',\n",
       " 600: 'super',\n",
       " 601: 'third',\n",
       " 602: 'either',\n",
       " 603: 'dies',\n",
       " 604: 'hours',\n",
       " 605: 'murder',\n",
       " 606: 'excited',\n",
       " 607: 'wallace',\n",
       " 608: 'maybe',\n",
       " 609: 'side',\n",
       " 610: 'jail',\n",
       " 611: '50',\n",
       " 612: 'foundation',\n",
       " 613: 'trump2016',\n",
       " 614: 'pm',\n",
       " 615: 'less',\n",
       " 616: 'facts',\n",
       " 617: 'lives',\n",
       " 618: 'town',\n",
       " 619: 'plans',\n",
       " 620: 'attacks',\n",
       " 621: 'playing',\n",
       " 622: 'using',\n",
       " 623: 'forward',\n",
       " 624: 'cleveland',\n",
       " 625: 'leave',\n",
       " 626: 'three',\n",
       " 627: 'safe',\n",
       " 628: 'wish',\n",
       " 629: 'china',\n",
       " 630: 'drinking',\n",
       " 631: 'accused',\n",
       " 632: 'low',\n",
       " 633: 'sad',\n",
       " 634: 'email',\n",
       " 635: 'used',\n",
       " 636: 'list',\n",
       " 637: 'makeamericagreatagain',\n",
       " 638: 'msm',\n",
       " 639: 'sick',\n",
       " 640: 'proud',\n",
       " 641: 'stage',\n",
       " 642: 'justice',\n",
       " 643: 'climate',\n",
       " 644: 'winning',\n",
       " 645: 'few',\n",
       " 646: 'refugees',\n",
       " 647: 'violence',\n",
       " 648: 'focus',\n",
       " 649: 'told',\n",
       " 650: 'Å¡',\n",
       " 651: 'instead',\n",
       " 652: 'else',\n",
       " 653: 'knows',\n",
       " 654: 'ok',\n",
       " 655: '11',\n",
       " 656: 'guy',\n",
       " 657: 'blacklivesmatter',\n",
       " 658: 'once',\n",
       " 659: 'word',\n",
       " 660: 'students',\n",
       " 661: '12',\n",
       " 662: 'rest',\n",
       " 663: 'speak',\n",
       " 664: 'judge',\n",
       " 665: 'share',\n",
       " 666: 'having',\n",
       " 667: 'fox',\n",
       " 668: 'become',\n",
       " 669: 'russian',\n",
       " 670: 'sign',\n",
       " 671: 'tweets',\n",
       " 672: 'baby',\n",
       " 673: 'republicans',\n",
       " 674: 'outside',\n",
       " 675: 'college',\n",
       " 676: 'water',\n",
       " 677: 'paid',\n",
       " 678: 'such',\n",
       " 679: 'dnc',\n",
       " 680: 'bless',\n",
       " 681: 'press',\n",
       " 682: 'whole',\n",
       " 683: 'folks',\n",
       " 684: 'anti',\n",
       " 685: 'nyc',\n",
       " 686: 'special',\n",
       " 687: 'miss',\n",
       " 688: 'elections',\n",
       " 689: \"they're\",\n",
       " 690: 'wonder',\n",
       " 691: 'asked',\n",
       " 692: 'warriors',\n",
       " 693: 'elected',\n",
       " 694: 'cruz',\n",
       " 695: 'red',\n",
       " 696: 'prison',\n",
       " 697: 'choose',\n",
       " 698: 'wednesdaywisdom',\n",
       " 699: 'fair',\n",
       " 700: 'liberal',\n",
       " 701: 'ban',\n",
       " 702: 'happen',\n",
       " 703: 'young',\n",
       " 704: 'kill',\n",
       " 705: 'guess',\n",
       " 706: 'c',\n",
       " 707: 'vote2016',\n",
       " 708: 'together',\n",
       " 709: '3rd',\n",
       " 710: 'pray',\n",
       " 711: 'senate',\n",
       " 712: 'calling',\n",
       " 713: 'john',\n",
       " 714: 'victims',\n",
       " 715: 'officials',\n",
       " 716: 'cannot',\n",
       " 717: 'comes',\n",
       " 718: 'charged',\n",
       " 719: 'air',\n",
       " 720: 'student',\n",
       " 721: 'strong',\n",
       " 722: 'won',\n",
       " 723: 'injured',\n",
       " 724: 'fun',\n",
       " 725: 'problem',\n",
       " 726: 'killing',\n",
       " 727: 'biggest',\n",
       " 728: 'beat',\n",
       " 729: 'heart',\n",
       " 730: 'muslims',\n",
       " 731: 'human',\n",
       " 732: 'others',\n",
       " 733: 'sexual',\n",
       " 734: 'half',\n",
       " 735: 'die',\n",
       " 736: 'agree',\n",
       " 737: 'nice',\n",
       " 738: 'evil',\n",
       " 739: \"who's\",\n",
       " 740: \"clinton's\",\n",
       " 741: 'results',\n",
       " 742: 'ccot',\n",
       " 743: 'govote',\n",
       " 744: \"you've\",\n",
       " 745: 'son',\n",
       " 746: 'border',\n",
       " 747: 'freedom',\n",
       " 748: \"haven't\",\n",
       " 749: 'reason',\n",
       " 750: 'york',\n",
       " 751: 'each',\n",
       " 752: 'move',\n",
       " 753: 'ago',\n",
       " 754: 'west',\n",
       " 755: 'protect',\n",
       " 756: 'hands',\n",
       " 757: 'means',\n",
       " 758: 'ivoted',\n",
       " 759: 'peace',\n",
       " 760: 'center',\n",
       " 761: 'front',\n",
       " 762: 'mind',\n",
       " 763: 'order',\n",
       " 764: 'claims',\n",
       " 765: 'obamacare',\n",
       " 766: 'd',\n",
       " 767: 'fighting',\n",
       " 768: 'chief',\n",
       " 769: 'haiti',\n",
       " 770: 'dog',\n",
       " 771: 'mom',\n",
       " 772: 'washington',\n",
       " 773: 'dc',\n",
       " 774: 'took',\n",
       " 775: 'talks',\n",
       " 776: 'democrat',\n",
       " 777: 'community',\n",
       " 778: 'release',\n",
       " 779: 'major',\n",
       " 780: 'music',\n",
       " 781: 'arrest',\n",
       " 782: 'yeah',\n",
       " 783: 'group',\n",
       " 784: 'votetrump',\n",
       " 785: 'tune',\n",
       " 786: 'im',\n",
       " 787: 'body',\n",
       " 788: 'rape',\n",
       " 789: 'praying',\n",
       " 790: 'putin',\n",
       " 791: 'nuclear',\n",
       " 792: 'soon',\n",
       " 793: 'issue',\n",
       " 794: 'lets',\n",
       " 795: 'o',\n",
       " 796: 'kind',\n",
       " 797: 'pretty',\n",
       " 798: 'fall',\n",
       " 799: 'probably',\n",
       " 800: 'clintons',\n",
       " 801: 'islamic',\n",
       " 802: 'reality',\n",
       " 803: 'four',\n",
       " 804: 'send',\n",
       " 805: 'hall',\n",
       " 806: 'caught',\n",
       " 807: 'guys',\n",
       " 808: 'mexico',\n",
       " 809: 'huge',\n",
       " 810: 'inside',\n",
       " 811: 'facebook',\n",
       " 812: 'himself',\n",
       " 813: 'officers',\n",
       " 814: 'iran',\n",
       " 815: 'elect',\n",
       " 816: 'mother',\n",
       " 817: 'families',\n",
       " 818: 'dear',\n",
       " 819: 'hold',\n",
       " 820: 'assault',\n",
       " 821: 'hurricane',\n",
       " 822: \"y'all\",\n",
       " 823: 'expect',\n",
       " 824: \"we'll\",\n",
       " 825: 'protest',\n",
       " 826: 'poor',\n",
       " 827: '2a',\n",
       " 828: 'federal',\n",
       " 829: 'driver',\n",
       " 830: 'street',\n",
       " 831: \"you'll\",\n",
       " 832: 'possible',\n",
       " 833: \"i'd\",\n",
       " 834: 'clara',\n",
       " 835: 'democratic',\n",
       " 836: 'thinks',\n",
       " 837: 'class',\n",
       " 838: 'course',\n",
       " 839: 'teen',\n",
       " 840: 'service',\n",
       " 841: 'cause',\n",
       " 842: 'blue',\n",
       " 843: 'official',\n",
       " 844: 'presidentialdebate',\n",
       " 845: 'amazing',\n",
       " 846: 'middle',\n",
       " 847: 'break',\n",
       " 848: 'tells',\n",
       " 849: 'stick',\n",
       " 850: 'storm',\n",
       " 851: 'funny',\n",
       " 852: 'wanted',\n",
       " 853: 'act',\n",
       " 854: 'worldnews',\n",
       " 855: 'food',\n",
       " 856: 'presidency',\n",
       " 857: 'app',\n",
       " 858: 'korea',\n",
       " 859: 'father',\n",
       " 860: 'second',\n",
       " 861: 'joke',\n",
       " 862: 'friend',\n",
       " 863: 'gives',\n",
       " 864: 'ny',\n",
       " 865: 'immigrants',\n",
       " 866: 'l',\n",
       " 867: 'brother',\n",
       " 868: 'train',\n",
       " 869: 'swamp',\n",
       " 870: 'kaine',\n",
       " 871: 'ex',\n",
       " 872: 'deplorable',\n",
       " 873: 'crazy',\n",
       " 874: 'leaders',\n",
       " 875: 'duty',\n",
       " 876: 'worse',\n",
       " 877: 'station',\n",
       " 878: 'wife',\n",
       " 879: 'damn',\n",
       " 880: 'hour',\n",
       " 881: 'photos',\n",
       " 882: 'citizens',\n",
       " 883: 'foreign',\n",
       " 884: 'building',\n",
       " 885: 'gt',\n",
       " 886: 'season',\n",
       " 887: 'idea',\n",
       " 888: 'prep',\n",
       " 889: 'beautiful',\n",
       " 890: 'h',\n",
       " 891: 'happens',\n",
       " 892: 'lady',\n",
       " 893: 'respect',\n",
       " 894: 'across',\n",
       " 895: 'worst',\n",
       " 896: 'photo',\n",
       " 897: 'fear',\n",
       " 898: 'worth',\n",
       " 899: 'bus',\n",
       " 900: 'started',\n",
       " 901: 'gov',\n",
       " 902: 'perfect',\n",
       " 903: 'due',\n",
       " 904: 'interesting',\n",
       " 905: 'hilary',\n",
       " 906: 'guilty',\n",
       " 907: 'daughter',\n",
       " 908: 'almost',\n",
       " 909: 'entire',\n",
       " 910: 'retweet',\n",
       " 911: 'cool',\n",
       " 912: 'moderator',\n",
       " 913: 'parents',\n",
       " 914: 'cover',\n",
       " 915: 'workers',\n",
       " 916: 'seriously',\n",
       " 917: 'room',\n",
       " 918: 'sf',\n",
       " 919: 'dem',\n",
       " 920: 'learn',\n",
       " 921: 'loser',\n",
       " 922: 'plane',\n",
       " 923: 'israel',\n",
       " 924: 'disgusting',\n",
       " 925: 'gave',\n",
       " 926: 'went',\n",
       " 927: 'book',\n",
       " 928: \"aren't\",\n",
       " 929: 'mayor',\n",
       " 930: 'meet',\n",
       " 931: 'giving',\n",
       " 932: 'fix',\n",
       " 933: 'mike',\n",
       " 934: 'education',\n",
       " 935: 'force',\n",
       " 936: 'small',\n",
       " 937: 'matters',\n",
       " 938: 'threat',\n",
       " 939: 'clear',\n",
       " 940: 'hot',\n",
       " 941: 'wakeupamerica',\n",
       " 942: 'islam',\n",
       " 943: 'syrian',\n",
       " 944: 'buy',\n",
       " 945: 'bitch',\n",
       " 946: 'terrorist',\n",
       " 947: 'self',\n",
       " 948: 'nowplaying',\n",
       " 949: 'terrorism',\n",
       " 950: 'et',\n",
       " 951: '2017',\n",
       " 952: 'weather',\n",
       " 953: 'pick',\n",
       " 954: 'count',\n",
       " 955: 'bowl',\n",
       " 956: 'rise',\n",
       " 957: 'drug',\n",
       " 958: 'valley',\n",
       " 959: 'standing',\n",
       " 960: 'rich',\n",
       " 961: 'following',\n",
       " 962: 'star',\n",
       " 963: 'seems',\n",
       " 964: 'honest',\n",
       " 965: 'terror',\n",
       " 966: 'daily',\n",
       " 967: 'guns',\n",
       " 968: 'meeting',\n",
       " 969: 'living',\n",
       " 970: 'held',\n",
       " 971: 'ad',\n",
       " 972: 'event',\n",
       " 973: 'november',\n",
       " 974: 'along',\n",
       " 975: 'fired',\n",
       " 976: 'imagine',\n",
       " 977: 'sorry',\n",
       " 978: 'awesome',\n",
       " 979: 'failed',\n",
       " 980: 'victim',\n",
       " 981: 'continue',\n",
       " 982: 'test',\n",
       " 983: 'laws',\n",
       " 984: 'f',\n",
       " 985: 'thousands',\n",
       " 986: 'secret',\n",
       " 987: \"wouldn't\",\n",
       " 988: 'johnson',\n",
       " 989: 'returns',\n",
       " 990: 'army',\n",
       " 991: 'feeling',\n",
       " 992: 'different',\n",
       " 993: 'j',\n",
       " 994: 'e',\n",
       " 995: 'losing',\n",
       " 996: 'shut',\n",
       " 997: 'tuesday',\n",
       " 998: 'term',\n",
       " 999: 'moment',\n",
       " 1000: 'personal',\n",
       " ...}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleLSTM():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words, 125,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64) (layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 150, 125)          1250000   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                48640     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,315,537\n",
      "Trainable params: 1,315,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SimpleLSTM()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100977 samples, validate on 25245 samples\n",
      "Epoch 1/10\n",
      "100977/100977 [==============================] - 199s 2ms/step - loss: 0.2639 - acc: 0.8804 - val_loss: 0.2171 - val_acc: 0.9033\n",
      "Epoch 2/10\n",
      "100977/100977 [==============================] - 198s 2ms/step - loss: 0.2009 - acc: 0.9132 - val_loss: 0.2110 - val_acc: 0.9067\n",
      "Epoch 3/10\n",
      "100977/100977 [==============================] - 196s 2ms/step - loss: 0.1812 - acc: 0.9241 - val_loss: 0.2090 - val_acc: 0.9089\n",
      "Epoch 4/10\n",
      "100977/100977 [==============================] - 268s 3ms/step - loss: 0.1651 - acc: 0.9320 - val_loss: 0.2152 - val_acc: 0.9071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a9c6d2908>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22275/22275 [==============================] - 17s 756us/step\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.217\n",
      "  Accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words, 125,input_length=max_len)(inputs)\n",
    "    layer = Conv1D(64, 3, activation='relu') (layer)\n",
    "    layer = Conv1D(64, 3, activation='relu')(layer)\n",
    "    layer = MaxPool1D(pool_size=3) (layer)\n",
    "    layer = Conv1D(128, 3, activation='relu') (layer)\n",
    "    layer = Conv1D(128, 3, activation='relu') (layer)\n",
    "    layer = GlobalAveragePooling1D() (layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 150, 125)          1250000   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 148, 64)           24064     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 146, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 46, 128)           24704     \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 44, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,360,529\n",
      "Trainable params: 1,360,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100977 samples, validate on 25245 samples\n",
      "Epoch 1/10\n",
      "100977/100977 [==============================] - 143s 1ms/step - loss: 0.4046 - acc: 0.8123 - val_loss: 0.3100 - val_acc: 0.8555\n",
      "Epoch 2/10\n",
      "100977/100977 [==============================] - 139s 1ms/step - loss: 0.2959 - acc: 0.8684 - val_loss: 0.2928 - val_acc: 0.8695\n",
      "Epoch 3/10\n",
      "100977/100977 [==============================] - 146s 1ms/step - loss: 0.2767 - acc: 0.8812 - val_loss: 0.2915 - val_acc: 0.8716\n",
      "Epoch 4/10\n",
      "100977/100977 [==============================] - 144s 1ms/step - loss: 0.2612 - acc: 0.8891 - val_loss: 0.2885 - val_acc: 0.8732\n",
      "Epoch 5/10\n",
      "100977/100977 [==============================] - 143s 1ms/step - loss: 0.2476 - acc: 0.8958 - val_loss: 0.2940 - val_acc: 0.8692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a9c814d68>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22275/22275 [==============================] - 9s 411us/step\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.303\n",
      "  Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLSTM():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words, 125,input_length=max_len)(inputs)\n",
    "    layer = Conv1D(64, 3, activation='relu') (layer)\n",
    "    layer = Conv1D(64, 3, activation='relu')(layer)\n",
    "    layer = MaxPool1D(pool_size=3) (layer)\n",
    "    layer = LSTM(64) (layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 150, 125)          1250000   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 148, 64)           24064     \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 146, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,336,337\n",
      "Trainable params: 1,336,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CLSTM()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100977 samples, validate on 25245 samples\n",
      "Epoch 1/10\n",
      "100977/100977 [==============================] - 151s 1ms/step - loss: 0.3419 - acc: 0.8427 - val_loss: 0.3029 - val_acc: 0.8668\n",
      "Epoch 2/10\n",
      "100977/100977 [==============================] - 157s 2ms/step - loss: 0.2779 - acc: 0.8787 - val_loss: 0.2909 - val_acc: 0.8718\n",
      "Epoch 3/10\n",
      " 25600/100977 [======>.......................] - ETA: 1:42 - loss: 0.2535 - acc: 0.8916"
     ]
    }
   ],
   "source": [
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
